{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from classification_plots import plot_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from collections import Counter\n",
    "from sklearn.multiclass import OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Able to specify which GPU to use\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "orig_SR = 44100\n",
    "orig_blocksize = int(orig_SR * 5)\n",
    "orig_overlap = 0 #int(orig_SR/4)\n",
    "\n",
    "SR = 16000\n",
    "blocksize = int(SR * 5)\n",
    "overlap = 0 #int(SR/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_dir = '../../ESC-50/audio/'\n",
    "path_to_db='../../ESC-50/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset\n",
    "Here we load the csv that describes each file in the dataset. We add a high level category that is defined in the ESC-50 documentation. This we realize is anthetical to true training, it is a stopgap for when we use NLP to classify tags into these categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(path_to_db + 'meta/esc50.csv')\n",
    "classes = [None] * 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_classes = ['Human & Animal', 'Interacting Materials']\n",
    "mapping = {'dog': 0,'rooster': 0,'pig': 0,'cow': 0,'frog': 0,'cat': 0,'hen': 0,\n",
    "            'insects': 0,'sheep': 0,'crow': 0,'rain': 1,'sea_waves': 1,'crackling_fire': 1,\n",
    "            'crickets': 0,'chirping_birds': 0,'water_drops': 1,'wind': 1,'pouring_water': 1,\n",
    "            'toilet_flush': 1,'thunderstorm': 1,'crying_baby': 0,'sneezing': 0,'clapping': 0,\n",
    "            'breathing': 0,'coughing': 0,'footsteps': 1,'laughing': 0,'brushing_teeth': 1,\n",
    "            'snoring': 0,'drinking_sipping': 1,'door_wood_knock': 1,'mouse_click': 1,\n",
    "            'keyboard_typing': 1,'door_wood_creaks': 1,'can_opening': 1,'washing_machine': 1,\n",
    "            'vacuum_cleaner': 1,'clock_alarm': 1,'clock_tick': 1,'glass_breaking':1,'helicopter': 1,\n",
    "            'chainsaw': 1,'siren': 1,'car_horn': 1,'engine': 1,'train': 1,'church_bells': 1,\n",
    "            'airplane': 1,'fireworks': 1,'hand_saw': 1,\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['h_target'] = None\n",
    "for index, row in dataset.iterrows():\n",
    "    target = row['target']\n",
    "    classes[target] = row['category']\n",
    "    dataset.loc[index, 'h_target'] = mapping[row['category']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>fold</th>\n",
       "      <th>target</th>\n",
       "      <th>category</th>\n",
       "      <th>esc10</th>\n",
       "      <th>src_file</th>\n",
       "      <th>take</th>\n",
       "      <th>h_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1-100032-A-0.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>dog</td>\n",
       "      <td>True</td>\n",
       "      <td>100032</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1-100038-A-14.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>chirping_birds</td>\n",
       "      <td>False</td>\n",
       "      <td>100038</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1-100210-A-36.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>vacuum_cleaner</td>\n",
       "      <td>False</td>\n",
       "      <td>100210</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1-100210-B-36.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>vacuum_cleaner</td>\n",
       "      <td>False</td>\n",
       "      <td>100210</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1-101296-A-19.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>thunderstorm</td>\n",
       "      <td>False</td>\n",
       "      <td>101296</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            filename  fold  target        category  esc10  src_file take  \\\n",
       "0   1-100032-A-0.wav     1       0             dog   True    100032    A   \n",
       "1  1-100038-A-14.wav     1      14  chirping_birds  False    100038    A   \n",
       "2  1-100210-A-36.wav     1      36  vacuum_cleaner  False    100210    A   \n",
       "3  1-100210-B-36.wav     1      36  vacuum_cleaner  False    100210    B   \n",
       "4  1-101296-A-19.wav     1      19    thunderstorm  False    101296    A   \n",
       "\n",
       "   h_target  \n",
       "0         0  \n",
       "1         0  \n",
       "2         1  \n",
       "3         1  \n",
       "4         1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting load_blockwise.py\n"
     ]
    }
   ],
   "source": [
    "%%file load_blockwise.py\n",
    "\n",
    "audio_dir = '../../ESC-50/audio/'\n",
    "path_to_db='../../ESC-50/'\n",
    "\n",
    "orig_SR = 44100\n",
    "orig_blocksize = int(orig_SR * 5)\n",
    "orig_overlap = 0 #int(orig_SR/4)\n",
    "\n",
    "SR = 16000\n",
    "blocksize = int(SR * 5)\n",
    "overlap = 0 #int(SR/4)\n",
    "\n",
    "import time\n",
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "\n",
    "def load_audio_blockwise(data, blocksize=1024, overlap=512, debug=False):\n",
    "    start_time = time.time()\n",
    "    items = []\n",
    "    target = []\n",
    "    h_target = []\n",
    "    for i, sample in data.iterrows():\n",
    "        if debug:\n",
    "            print(\"File Processing\", end=\"\", flush=True)\n",
    "        blockgen = sf.blocks(audio_dir + sample['filename'], \n",
    "                             blocksize=blocksize, \n",
    "                             overlap=overlap, \n",
    "                             always_2d=True,\n",
    "                             fill_value=0.0)\n",
    "        sr = sf.info(audio_dir + sample['filename']).samplerate\n",
    "        for bl in blockgen:\n",
    "            if not np.any(bl):\n",
    "                continue\n",
    "            if debug:\n",
    "                print(\".\", end=\"\", flush=True)\n",
    "            y = bl.transpose()\n",
    "            y = librosa.resample(y, sr, SR)\n",
    "            y = y[:int(blocksize)]\n",
    "            y = y[np.newaxis, :]\n",
    "            items.append(y)\n",
    "            h_target.append(sample.h_target)\n",
    "            target.append(sample.target)\n",
    "        if debug:\n",
    "            print(\"Done\")\n",
    "    if debug:\n",
    "        print(\"\\tProcessing Time: \" + str(time.time() - start_time))\n",
    "    return np.vstack(items), np.array(h_target), np.array(target)\n",
    "\n",
    "def load_file_blockwise(filename, blocksize=1024, overlap=512, debug=False):\n",
    "    items = []\n",
    "    if debug:\n",
    "        print(\"File Processing\", end=\"\", flush=True)\n",
    "    blockgen = sf.blocks(audio_dir + filename, \n",
    "                         blocksize=blocksize, \n",
    "                         overlap=overlap, \n",
    "                         always_2d=True,\n",
    "                         fill_value=0.0)\n",
    "    sr = sf.info(audio_dir + filename).samplerate\n",
    "    for bl in blockgen:\n",
    "        if not np.any(bl):\n",
    "            continue\n",
    "        if debug:\n",
    "            print(\".\", end=\"\", flush=True)\n",
    "        y = bl.transpose()\n",
    "        y = librosa.resample(y, sr, SR)\n",
    "        y = y[:int(blocksize)]\n",
    "        y = y[np.newaxis, :]\n",
    "        items.append(y)\n",
    "        \n",
    "    if debug:\n",
    "        print(\"Done\")\n",
    "\n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 2404.05 MiB, increment: 2099.29 MiB\n",
      "CPU times: user 4min 1s, sys: 1.55 s, total: 4min 3s\n",
      "Wall time: 4min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from load_blockwise import load_audio_blockwise\n",
    "\n",
    "%memit train_X, train_y, train_yy = load_audio_blockwise(dataset[dataset.fold != 1], orig_blocksize, orig_overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 1731.67 MiB, increment: 432.70 MiB\n",
      "CPU times: user 1min 1s, sys: 468 ms, total: 1min 1s\n",
      "Wall time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%memit test_X, test_y, test_yy = load_audio_blockwise(dataset[dataset.fold == 1], orig_blocksize, orig_overlap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Convolution2D, MaxPooling2D, Flatten, InputLayer\n",
    "import keras.metrics as kmet\n",
    "from kapre.time_frequency import Melspectrogram, Spectrogram\n",
    "from kapre.utils import Normalization2D\n",
    "from kapre.augmentation import AdditiveNoise\n",
    "\n",
    "ensemble_num = 10\n",
    "num_hidden_neurons = 10\n",
    "dropout = 0.25\n",
    "\n",
    "epochs = 5\n",
    "batch = 128\n",
    "\n",
    "def gpu_mfcc_shallow_net():\n",
    "    # Create Model\n",
    "    # Create Model\n",
    "    model = Sequential()\n",
    "    model.add(Melspectrogram(\n",
    "        sr=SR,\n",
    "        n_mels=128,\n",
    "        power_melgram=1.0,\n",
    "        input_shape=(1, blocksize),\n",
    "        trainable_fb=False,\n",
    "        fmin = 800,\n",
    "        fmax = 8000\n",
    "    ))\n",
    "    model.add(Convolution2D(32, 9, 9, name='conv1', activation='relu'))\n",
    "    model.add(MaxPooling2D((25, 17)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(2, kernel_initializer='normal', activation='softmax'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy', kmet.mae])\n",
    "    \n",
    "    model.summary()\n",
    "\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "melspectrogram_1 (Melspectro (None, 128, 313, 1)       296064    \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 120, 305, 32)      2624      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 4, 17, 32)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2176)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                69664     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 371,586\n",
      "Trainable params: 75,522\n",
      "Non-trainable params: 296,064\n",
      "_________________________________________________________________\n",
      "Train on 1360 samples, validate on 240 samples\n",
      "Epoch 1/5\n",
      "1360/1360 [==============================] - 77s 57ms/step - loss: 0.5867 - acc: 0.9206 - mean_absolute_error: 0.4319 - val_loss: 0.4652 - val_acc: 0.9667 - val_mean_absolute_error: 0.3027\n",
      "Epoch 2/5\n",
      "1360/1360 [==============================] - 74s 54ms/step - loss: 0.2665 - acc: 0.9824 - mean_absolute_error: 0.1620 - val_loss: 0.4901 - val_acc: 0.9667 - val_mean_absolute_error: 0.1046\n",
      "Epoch 3/5\n",
      "1360/1360 [==============================] - 71s 52ms/step - loss: 0.2285 - acc: 0.9824 - mean_absolute_error: 0.0561 - val_loss: 0.5076 - val_acc: 0.9667 - val_mean_absolute_error: 0.0564\n",
      "Epoch 4/5\n",
      "1360/1360 [==============================] - 69s 51ms/step - loss: 0.2171 - acc: 0.9824 - mean_absolute_error: 0.0349 - val_loss: 0.4737 - val_acc: 0.9667 - val_mean_absolute_error: 0.0550\n",
      "Epoch 5/5\n",
      "1360/1360 [==============================] - 69s 51ms/step - loss: 0.1710 - acc: 0.9824 - mean_absolute_error: 0.0493 - val_loss: 0.2992 - val_acc: 0.9667 - val_mean_absolute_error: 0.0867\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "melspectrogram_2 (Melspectro (None, 128, 313, 1)       296064    \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 120, 305, 32)      2624      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 17, 32)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 2176)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                69664     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 371,586\n",
      "Trainable params: 75,522\n",
      "Non-trainable params: 296,064\n",
      "_________________________________________________________________\n",
      "Train on 1360 samples, validate on 240 samples\n",
      "Epoch 1/5\n",
      "1360/1360 [==============================] - 70s 51ms/step - loss: 0.6051 - acc: 0.9081 - mean_absolute_error: 0.4454 - val_loss: 0.4021 - val_acc: 0.9875 - val_mean_absolute_error: 0.3074\n",
      "Epoch 2/5\n",
      "1280/1360 [===========================>..] - ETA: 3s - loss: 0.3374 - acc: 0.9773 - mean_absolute_error: 0.1876"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "clas = OneVsRestClassifier(KerasClassifier(build_fn=gpu_mfcc_shallow_net,\n",
    "                       epochs=epochs, \n",
    "                       batch_size=batch, \n",
    "                       validation_split=0.15))\n",
    "\n",
    "%memit history = clas.fit(train_X, train_yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "pred = clas.predict(test_X)\n",
    "print(metrics.accuracy_score(test_yy, pred))\n",
    "cm = metrics.confusion_matrix(test_yy, pred)\n",
    "plt.figure(figsize=(10,10))\n",
    "plot_confusion_matrix(cm, classes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Evaluation\n",
    "We combine the classifiers to determine overall performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import time\n",
    "from load_blockwise import load_file_blockwise\n",
    "\n",
    "predictions = []\n",
    "full_targets = []\n",
    "start_time = time.time()\n",
    "top_pred_probs = []\n",
    "pred_probs = []\n",
    "\n",
    "for data_iloc in range(0,2000):\n",
    "    s_time = time.time()\n",
    "    x_file = load_file_blockwise(dataset.iloc[data_iloc].filename,\n",
    "                                 blocksize=orig_blocksize,\n",
    "                                 overlap=orig_overlap)\n",
    "    y_file = dataset.iloc[data_iloc].h_target\n",
    "    yy_file = dataset.iloc[data_iloc].target\n",
    "    \n",
    "    pred = clas.predict(x_file)\n",
    "    pred_probs.append(clas.predict_proba(x_file))\n",
    "    b = Counter(pred)\n",
    "    predictions.append(b.most_common(1)[0][0])\n",
    "   \n",
    "    full_targets.append(yy_file)\n",
    "    print(\"\\tFile Time: \" + str(time.time() - s_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.accuracy_score(full_targets, predictions))\n",
    "print(metrics.precision_score(full_targets, predictions, average='macro'))\n",
    "cm = metrics.confusion_matrix(full_targets, predictions)\n",
    "plt.figure(figsize=(20,20))\n",
    "plot_confusion_matrix(cm, classes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = OneVsRestClassifier(KerasClassifier(build_fn=gpu_mfcc_shallow_net,\n",
    "                       epochs=epochs, \n",
    "                       batch_size=batch, \n",
    "                       validation_split=0.15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "scores = cross_validate(clf, train_X, train_yy, cv=3, scoring=['accuracy', 'precision_macro', 'recall_macro'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf.fit(train_X, train_yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heuristic(score_array):\n",
    "    mu_t = np.mean(score_array)\n",
    "    std_t = np.std(score_array)\n",
    "    for i in range(0, len(score_array)):\n",
    "        score_array[i] = (score_array[i] - mu_t)/std_t\n",
    "    return score_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_dataset(query_term):\n",
    "    h_l = mapping[query_term]\n",
    "    l_l = classes.index(query_term)\n",
    "\n",
    "    predictions = []\n",
    "    test = df[df.fold == 1]\n",
    "\n",
    "    for data_iloc in range(0,len(X_test)):\n",
    "        x_file = X_test[X_test.index == data_iloc]\n",
    "    \n",
    "        y_file = y_test[data_iloc]\n",
    "        \n",
    "        y_file = dataset.iloc[data_iloc].h_target\n",
    "        yy_file = dataset.iloc[data_iloc].target\n",
    "\n",
    "        predictions.append({\n",
    "            'file': dataset.iloc[data_iloc].filename,\n",
    "            'ds_id': data_iloc,\n",
    "            'prob': np.average(clf.predict_proba(x_file)[:,l_l]),\n",
    "            'prediction': clf.predict(x_file),\n",
    "            'decision_funct': heuristic(clf.decision_function(x_file)[0])[l_l],\n",
    "        })\n",
    "    \n",
    "    predictions = pd.DataFrame(predictions).sort_values(by=['decision_funct'], ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "preds = query_dataset('dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking index\n",
    "def check_accuracy(preds, query, count):\n",
    "    misclass = []\n",
    "    \n",
    "    print(query)\n",
    "    for data_iloc in range(0, count):\n",
    "        row = preds.iloc[data_iloc]\n",
    "        if dataset.iloc[row.ds_id].target != classes.index(query):\n",
    "            misclass.append(classes[dataset.iloc[row.ds_id].target])\n",
    "            \n",
    "    print(misclass)\n",
    "    print()\n",
    "    return (count - len(misclass))/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking index\n",
    "check_accuracy(preds, 'dog', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "for cls in classes:\n",
    "    preds = query_dataset(cls)\n",
    "    acc = check_accuracy(preds, cls, 10)\n",
    "    print(acc)\n",
    "    accuracies.append(acc)\n",
    "    \n",
    "np.average(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
