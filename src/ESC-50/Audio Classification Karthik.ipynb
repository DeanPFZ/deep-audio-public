{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "#from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from classification_plots import plot_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import scipy \n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_dir = '../../ESC-50/audio/'\n",
    "path_to_db='../../ESC-50/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 2000\n",
      "./data1/class29\\2-94807-A-29.wav\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import librosa\n",
    "fs = 44100\n",
    "class audiotrainset(Dataset):\n",
    "    def __init__(self):\n",
    "        \n",
    "        root = \"./data1/\"\n",
    "        self.data_list = []\n",
    "        self.label_list = []\n",
    "        for root, dir, files in os.walk(\"./data1/\"):\n",
    "            for file in files:\n",
    "                if file.find('.wav')!= -1:\n",
    "#                     print(os.path.join(root, file))\n",
    "#                     print(int(root[13:]))\n",
    "                    self.data_list.append(os.path.join(root, file))\n",
    "                    self.label_list.append(int(root[13:]))\n",
    "        print(len(self.data_list), len(self.label_list))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        path = self.data_list[index]\n",
    "        data,sr = librosa.core.load(path)\n",
    "#         data = np.expand_dims(data, axis=0)\n",
    "        label= self.label_list[index] \n",
    "\n",
    "        label = np.asarray(label)\n",
    "        \n",
    "        return data, label,path\n",
    "audiodataloader = audiotrainset()\n",
    "data,label,path = audiodataloader.__getitem__(895)\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extracts LPC coefficients ###\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.signal as sig\n",
    "\n",
    "def lpc_ref(signal, order):\n",
    "    \"\"\"Compute the Linear Prediction Coefficients.\n",
    "    Return the order + 1 LPC coefficients for the signal. c = lpc(x, k) will\n",
    "    find the k+1 coefficients of a k order linear filter:\n",
    "      xp[n] = -c[1] * x[n-2] - ... - c[k-1] * x[n-k-1]\n",
    "    Such as the sum of the squared-error e[i] = xp[i] - x[i] is minimized.\n",
    "    Parameters\n",
    "    ----------\n",
    "    signal: array_like\n",
    "        input signal\n",
    "    order : int\n",
    "        LPC order (the output will have order + 1 items)\n",
    "    Note\n",
    "    ----\n",
    "    This is just for reference, as it is using the direct inversion of the\n",
    "    toeplitz matrix, which is really slow\"\"\"\n",
    "    if signal.ndim > 1:\n",
    "        raise ValueError(\"Array of rank > 1 not supported yet\")\n",
    "#     if order > signal.size:\n",
    "#         raise ValueError(\"Input signal must have a lenght >= lpc order\")\n",
    "\n",
    "    if order > 0:\n",
    "        p = order + 1\n",
    "        r = np.zeros(p, signal.dtype)\n",
    "        # Number of non zero values in autocorrelation one needs for p LPC\n",
    "        # coefficients\n",
    "        nx = np.min([p, signal.size])\n",
    "        x = np.correlate(signal, signal, 'full')\n",
    "        r[:nx] = x[signal.size-1:signal.size+order]\n",
    "        phi = np.dot(sp.linalg.inv(sp.linalg.toeplitz(r[:-1])), -r[1:])\n",
    "        return np.concatenate(([1.], phi))\n",
    "    else:\n",
    "        return np.ones(1, dtype = signal.dtype)\n",
    "\n",
    "def levinson_1d(r, order):\n",
    "    \"\"\"Levinson-Durbin recursion, to efficiently solve symmetric linear systems\n",
    "    with toeplitz structure.\n",
    "    Arguments\n",
    "    ---------\n",
    "        r : array-like\n",
    "            input array to invert (since the matrix is symmetric Toeplitz, the\n",
    "            corresponding pxp matrix is defined by p items only). Generally the\n",
    "            autocorrelation of the signal for linear prediction coefficients\n",
    "            estimation. The first item must be a non zero real.\n",
    "    Note\n",
    "    ----\n",
    "    This implementation is in python, hence unsuitable for any serious\n",
    "    computation. Use it as educational and reference purpose only.\n",
    "    Levinson is a well-known algorithm to solve the Hermitian toeplitz\n",
    "    equation:\n",
    "                       _          _\n",
    "        -R[1] = R[0]   R[1]   ... R[p-1]    a[1]\n",
    "         :      :      :          :      *  :\n",
    "         :      :      :          _      *  :\n",
    "        -R[p] = R[p-1] R[p-2] ... R[0]      a[p]\n",
    "                       _\n",
    "    with respect to a (  is the complex conjugate). Using the special symmetry\n",
    "    in the matrix, the inversion can be done in O(p^2) instead of O(p^3).  \n",
    "    \"\"\"\n",
    "    r = np.atleast_1d(r)\n",
    "    if r.ndim > 1:\n",
    "        raise ValueError(\"Only rank 1 are supported for now.\")\n",
    "\n",
    "    n = r.size\n",
    "    if order > n - 1:\n",
    "        raise ValueError(\"Order should be <= size-1\")\n",
    "    elif n < 1:\n",
    "        raise ValueError(\"Cannot operate on empty array !\")\n",
    "\n",
    "    if not np.isreal(r[0]):\n",
    "        raise ValueError(\"First item of input must be real.\")\n",
    "    elif not np.isfinite(1/r[0]):\n",
    "        raise ValueError(\"First item should be != 0\")\n",
    "\n",
    "    # Estimated coefficients\n",
    "    a = np.empty(order+1, r.dtype)\n",
    "    # temporary array\n",
    "    t = np.empty(order+1, r.dtype)\n",
    "    # Reflection coefficients\n",
    "    k = np.empty(order, r.dtype)\n",
    "\n",
    "    a[0] = 1.\n",
    "    e = r[0]\n",
    "\n",
    "    for i in xrange(1, order+1):\n",
    "        acc = r[i]\n",
    "        for j in range(1, i):\n",
    "            acc += a[j] * r[i-j]\n",
    "        k[i-1] = -acc / e\n",
    "        a[i] = k[i-1]\n",
    "\n",
    "        for j in range(order):\n",
    "            t[j] = a[j]\n",
    "\n",
    "        for j in range(1, i):\n",
    "            a[j] += k[i-1] * np.conj(t[i-j])\n",
    "\n",
    "        e *= 1 - k[i-1] * np.conj(k[i-1])\n",
    "\n",
    "    return a, e, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Wavelet Decomposition #######\n",
    "import pywt\n",
    "def wavelet_decompose(y,wtype):\n",
    "    wt = pywt.Wavelet(wtype)\n",
    "    cA = pywt.wavedec(y,wt,level=2)[0]\n",
    "    cA = np.asarray(cA)\n",
    "    return cA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Stats from a signal #####\n",
    "def get_stats(y):\n",
    "    split_array = np.array_split(y,4)\n",
    "    stats_vector = np.zeros(len(split_array)*2)\n",
    "    offset = 0\n",
    "    for idx,chunk in enumerate(split_array,0):\n",
    "        stats_vector[offset] = np.mean(chunk)\n",
    "        stats_vector[offset + 1] = np.var(chunk)\n",
    "        offset = offset + 2\n",
    "    stats_vector = np.asarray(stats_vector)\n",
    "    return (stats_vector)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### Extract MFCC Coefficients #########\n",
    "def extract_mfcc(y,sr):\n",
    "    mfcc_array = librosa.feature.mfcc(y,sr,n_mfcc=13)\n",
    "    mfcc_features = []\n",
    "    for i in range(13):\n",
    "        mfcc_wav = wavelet_decompose(mfcc_array[i,:],'haar')\n",
    "        lpc_coeffs = lpc_ref(mfcc_wav,5)[1:]\n",
    "        mfcc_features.append(np.asarray(lpc_coeffs))\n",
    "    \n",
    "    \n",
    "    final_coeffs = 0\n",
    "    for coeff in mfcc_features:\n",
    "        final_coeffs = np.concatenate((final_coeffs,coeff),axis = None)\n",
    "    \n",
    "    final_coeffs = final_coeffs[1:]\n",
    "    \n",
    "    final_coeffs = np.asarray(final_coeffs) \n",
    "    return (final_coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(y):\n",
    "    sr = 44100\n",
    "    f1 = (lpc_ref(y,9)[1:])\n",
    "    f2 = (get_stats(wavelet_decompose(y,'haar')))\n",
    "    f3 = (extract_mfcc(y,sr))\n",
    "    f4 = (get_stats(wavelet_decompose(y,'db4')))\n",
    "    f5 = (get_stats(wavelet_decompose(y,'sym4')))\n",
    "    local_features =  np.concatenate((f1,f2,f3,f4,f5),axis = None)\n",
    "    return local_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpectralNet(\n",
      "  (fc1): Linear(in_features=98, out_features=200, bias=True)\n",
      "  (fc2): Linear(in_features=200, out_features=200, bias=True)\n",
      "  (fc3): Linear(in_features=200, out_features=50, bias=True)\n",
      "  (softmax): Softmax(dim=None)\n",
      "  (batchnorm1): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batchnorm2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SpectralNet(\n",
       "  (fc1): Linear(in_features=98, out_features=200, bias=True)\n",
       "  (fc2): Linear(in_features=200, out_features=200, bias=True)\n",
       "  (fc3): Linear(in_features=200, out_features=50, bias=True)\n",
       "  (softmax): Softmax(dim=None)\n",
       "  (batchnorm1): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batchnorm2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class SpectralNet(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(SpectralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(98,200)\n",
    "        self.fc2 = nn.Linear(200,200)\n",
    "        self.fc3 = nn.Linear(200,50)\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.batchnorm1 = nn.BatchNorm1d(200)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(200)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "model = SpectralNet()\n",
    "print (model)\n",
    "model.cuda()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 98)\n",
      "(5, 98)\n",
      "(5, 98)\n",
      "(5, 98)\n",
      "(5, 98)\n",
      "(5, 98)\n",
      "(5, 98)\n",
      "(5, 98)\n",
      "(5, 98)\n",
      "(5, 98)\n",
      "(5, 98)\n",
      "(5, 98)\n",
      "(5, 98)\n",
      "(5, 98)\n",
      "(5, 98)\n",
      "(5, 98)\n",
      "(5, 98)\n",
      "(5, 98)\n",
      "(5, 98)\n",
      "(5, 98)\n",
      "(5, 98)\n",
      "(5, 98)\n",
      "(5, 98)\n",
      "(5, 98)\n",
      "(5, 98)\n",
      "(5, 98)\n",
      "(5, 98)\n",
      "(5, 98)\n",
      "(5, 98)\n",
      "(5, 98)\n",
      "(5, 98)\n",
      "(5, 98)\n",
      "(5, 98)\n",
      "(5, 98)\n",
      "(5, 98)\n",
      "(5, 98)\n",
      "(5, 98)\n",
      "(5, 98)\n",
      "(5, 98)\n",
      "(5, 98)\n",
      "(5, 98)\n",
      "(5, 98)\n",
      "(5, 98)\n",
      "(5, 98)\n",
      "(5, 98)\n",
      "(5, 98)\n",
      "(5, 98)\n",
      "(5, 98)\n",
      "(5, 98)\n",
      "(5, 98)\n",
      "(5, 98)\n",
      "(5, 98)\n",
      "(5, 98)\n",
      "(5, 98)\n",
      "(5, 98)\n",
      "(5, 98)\n",
      "(5, 98)\n",
      "(5, 98)\n",
      "(5, 98)\n",
      "(5, 98)\n",
      "(5, 98)\n",
      "(5, 98)\n",
      "(5, 98)\n",
      "(5, 98)\n",
      "(5, 98)\n",
      "(5, 98)\n",
      "(5, 98)\n",
      "(5, 98)\n",
      "(5, 98)\n",
      "(5, 98)\n",
      "(5, 98)\n",
      "(5, 98)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-b2da663cf72c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpaths\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[0maudio\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeature_extraction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m             \u001b[0mbatch_feature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_feature\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-05dbd912cb02>\u001b[0m in \u001b[0;36mfeature_extraction\u001b[1;34m(y)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mfeature_extraction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0msr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m44100\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mf1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlpc_ref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mf2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mget_stats\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwavelet_decompose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'haar'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mf3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mextract_mfcc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-e45e32f28266>\u001b[0m in \u001b[0;36mlpc_ref\u001b[1;34m(signal, order)\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;31m# coefficients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mnx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msignal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorrelate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msignal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'full'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m         \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mnx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msignal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0msignal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mphi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoeplitz\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36mcorrelate\u001b[1;34m(a, v, mode)\u001b[0m\n\u001b[0;32m   1008\u001b[0m     \"\"\"\n\u001b[0;32m   1009\u001b[0m     \u001b[0mmode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_mode_from_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1010\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mmultiarray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorrelate2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1011\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1012\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "device = 0\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "validation_split = .25\n",
    "random_seed= 42\n",
    "shuffle_dataset = True\n",
    "dataset_size = 2000\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "dataset  = audiodataloader\n",
    "trainloader = torch.utils.data.DataLoader(dataset, batch_size=5, \n",
    "                                           sampler=train_sampler,pin_memory=True)\n",
    "valloader = torch.utils.data.DataLoader(dataset, batch_size=5,\n",
    "                                                sampler=valid_sampler,pin_memory=True)\n",
    "\n",
    "\n",
    "for epoch in range(400):  # loop over the dataset multiple times\n",
    "    \n",
    "    count = 0\n",
    "    validation_accuracy = 0\n",
    "    running_loss = 0.0\n",
    "    training_accuracy = 0\n",
    "    ######## training Stage ###########\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        count += 1\n",
    "        inputs, labels,paths = data\n",
    "        batch_feature = np.empty((0,98))\n",
    "        for path in paths:\n",
    "            audio,sr = librosa.core.load(path)\n",
    "            features = feature_extraction(audio)\n",
    "            batch_feature = np.vstack((batch_feature,features))\n",
    "            \n",
    "        \n",
    "        print(batch_feature.shape)\n",
    "        labels = labels.long().to(device)\n",
    "        batch_feature = torch.from_numpy(batch_feature).float().to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(batch_feature)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        acc_train = torch.eq(preds, labels).float().mean()\n",
    "        training_accuracy += acc_train.item()\n",
    "        if i % 100 == 99:\n",
    "            print(epoch, i, running_loss/100)\n",
    "            running_loss = 0\n",
    "        \n",
    "    epoch_loss = running_loss / count\n",
    "    print(\"epoch loss:\", epoch, epoch_loss)\n",
    "    print(\"train_accuracy:\", training_accuracy/count)\n",
    "    \n",
    "# #             torch.save(net, 'toy_model.pt')\n",
    "        \n",
    "    \n",
    "    print('Finished Training')    \n",
    "    running_loss = 0.0\n",
    "    count = 0\n",
    "    for i, data in enumerate(valloader, 0):\n",
    "        count += 1\n",
    "        inputs, labels,paths = data\n",
    "        batch_feature = np.empty((0,98))\n",
    "        for path in paths:\n",
    "            audio,sr = librosa.core.load(path)\n",
    "            features = feature_extraction(audio)\n",
    "            batch_feature = np.vstack((batch_feature,features))\n",
    "            \n",
    "        \n",
    "        print(batch_feature.shape)\n",
    "        labels = labels.long().to(device)\n",
    "        batch_feature = torch.from_numpy(batch_feature).float().to(device)\n",
    "        outputs = model(batch_feature)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        acc_val = torch.eq(preds, labels).float().mean()\n",
    "        validation_accuracy += acc_val.item()\n",
    "        \n",
    "    print(\"val_accuracy:\", validation_accuracy/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
