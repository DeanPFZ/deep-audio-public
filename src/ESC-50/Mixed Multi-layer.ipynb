{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/magenta/models/nsynth/wavenet/masked.py:115: UniformUnitScaling.__init__ (from tensorflow.python.ops.init_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.initializers.variance_scaling instead with distribution=uniform to get equivalent behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "import pandas as pd\n",
    "from preprocess import Audio_Processor\n",
    "from data_utils import balanced_supersample, balanced_subsample\n",
    "from sklearn import metrics\n",
    "from classification_plots import plot_confusion_matrix, plot_learning_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Able to specify which GPU to use\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "SR = 16000\n",
    "blocksize = int(SR/2)\n",
    "overlap = int(SR/4)\n",
    "\n",
    "orig_SR = 44100\n",
    "orig_blocksize = int(orig_SR * 5)\n",
    "orig_overlap = 0 #int(orig_SR/4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset\n",
    "Here we load the csv that describes each file in the dataset. We add a high level category that is defined in the ESC-50 documentation. This we realize is anthetical to true training, it is a stopgap for when we use NLP to classify tags into these categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_db='../../ESC-50/'\n",
    "audio_dir = path_to_db + 'audio/'\n",
    "ps = Audio_Processor(path_to_db + 'audio/', sr=SR)\n",
    "dataset = pd.read_csv(path_to_db + 'meta/esc50.csv')\n",
    "classes = [None] * 50\n",
    "h_classes = ['Human & Animal', 'Interacting Materials']\n",
    "mapping = {'dog': 0,'rooster': 0,'pig': 0,'cow': 0,'frog': 0,'cat': 0,'hen': 0,\n",
    "            'insects': 0,'sheep': 0,'crow': 0,'rain': 1,'sea_waves': 1,'crackling_fire': 1,\n",
    "            'crickets': 0,'chirping_birds': 0,'water_drops': 1,'wind': 1,'pouring_water': 1,\n",
    "            'toilet_flush': 1,'thunderstorm': 1,'crying_baby': 0,'sneezing': 0,'clapping': 0,\n",
    "            'breathing': 0,'coughing': 0,'footsteps': 1,'laughing': 0,'brushing_teeth': 1,\n",
    "            'snoring': 0,'drinking_sipping': 1,'door_wood_knock': 1,'mouse_click': 1,\n",
    "            'keyboard_typing': 1,'door_wood_creaks': 1,'can_opening': 1,'washing_machine': 1,\n",
    "            'vacuum_cleaner': 1,'clock_alarm': 1,'clock_tick': 1,'glass_breaking':1,'helicopter': 1,\n",
    "            'chainsaw': 1,'siren': 1,'car_horn': 1,'engine': 1,'train': 1,'church_bells': 1,\n",
    "            'airplane': 1,'fireworks': 1,'hand_saw': 1,\n",
    "            }\n",
    "dataset['h_target'] = None\n",
    "for index, row in dataset.iterrows():\n",
    "    target = row['target']\n",
    "    classes[target] = row['category']\n",
    "    dataset.loc[index, 'h_target'] = mapping[row['category']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "\n",
    "def load_audio_blockwise(data, blocksize=1024, overlap=512, debug=False):\n",
    "    start_time = time.time()\n",
    "    items = []\n",
    "    target = []\n",
    "    h_target = []\n",
    "    for i, sample in data.iterrows():\n",
    "        if debug:\n",
    "            print(\"File Processing\", end=\"\", flush=True)\n",
    "        blockgen = sf.blocks(audio_dir + sample['filename'], \n",
    "                             blocksize=blocksize, \n",
    "                             overlap=overlap, \n",
    "                             always_2d=True,\n",
    "                             fill_value=0.0)\n",
    "        sr = sf.info(audio_dir + sample['filename']).samplerate\n",
    "        for bl in blockgen:\n",
    "            if not np.any(bl):\n",
    "                continue\n",
    "            if debug:\n",
    "                print(\".\", end=\"\", flush=True)\n",
    "            y = bl.transpose()\n",
    "            y = librosa.resample(y, sr, SR)\n",
    "            y = y[:int(blocksize)]\n",
    "            y = y[np.newaxis, :]\n",
    "            items.append(y)\n",
    "            h_target.append(sample.h_target)\n",
    "            target.append(sample.target)\n",
    "        if debug:\n",
    "            print(\"Done\")\n",
    "    print(\"\\tProcessing Time: \" + str(time.time() - start_time))\n",
    "    return np.vstack(items), np.array(h_target), np.array(target)\n",
    "\n",
    "def load_file_blockwise(filename, blocksize=1024, overlap=512, debug=False):\n",
    "    items = []\n",
    "    if debug:\n",
    "        print(\"File Processing\", end=\"\", flush=True)\n",
    "    blockgen = sf.blocks(audio_dir + filename, \n",
    "                         blocksize=blocksize, \n",
    "                         overlap=overlap, \n",
    "                         always_2d=True,\n",
    "                         fill_value=0.0)\n",
    "    sr = sf.info(audio_dir + filename).samplerate\n",
    "    for bl in blockgen:\n",
    "        if not np.any(bl):\n",
    "            continue\n",
    "        if debug:\n",
    "            print(\".\", end=\"\", flush=True)\n",
    "        y = bl.transpose()\n",
    "        y = librosa.resample(y, sr, SR)\n",
    "        y = y[:int(blocksize)]\n",
    "        y = y[np.newaxis, :]\n",
    "        items.append(y)\n",
    "        \n",
    "    if debug:\n",
    "        print(\"Done\")\n",
    "\n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tProcessing Time: 299.20084166526794\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>h_target</th>\n",
       "      <th>derived</th>\n",
       "      <th>fold</th>\n",
       "      <th>base</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>mfcc_2_std  mfcc_2_mean  mfcc_2_noise  mfcc...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>mfcc_2_std  mfcc_2_mean  mfcc_2_noise  mfc...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.042354584992188686, -0.12557755392933506, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>mfcc_2_std  mfcc_2_mean  mfcc_2_noise  mfc...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.0071510895789658885, -0.008539486532633997...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>mfcc_2_std  mfcc_2_mean  mfcc_2_noise  mfc...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.3236247120233376, 0.23597562357962842, 0.49...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>mfcc_2_std  mfcc_2_mean  mfcc_2_noise  mfc...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.0004667657144710984, -0.000415740161992739...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target  h_target                                            derived  fold  \\\n",
       "0       0         0     mfcc_2_std  mfcc_2_mean  mfcc_2_noise  mfcc...     1   \n",
       "1      14         0      mfcc_2_std  mfcc_2_mean  mfcc_2_noise  mfc...     1   \n",
       "2      36         1      mfcc_2_std  mfcc_2_mean  mfcc_2_noise  mfc...     1   \n",
       "3      36         1      mfcc_2_std  mfcc_2_mean  mfcc_2_noise  mfc...     1   \n",
       "4      19         1      mfcc_2_std  mfcc_2_mean  mfcc_2_noise  mfc...     1   \n",
       "\n",
       "                                                base  \n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "1  [-0.042354584992188686, -0.12557755392933506, ...  \n",
       "2  [-0.0071510895789658885, -0.008539486532633997...  \n",
       "3  [0.3236247120233376, 0.23597562357962842, 0.49...  \n",
       "4  [-0.0004667657144710984, -0.000415740161992739...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_X, _, _ = load_audio_blockwise(dataset, orig_blocksize, orig_overlap)\n",
    "df = ps.preprocess_fold(dataset,\n",
    "                        kind='mfcc',\n",
    "                        blocksize=blocksize,\n",
    "                        overlap=overlap,\n",
    "                        feature_bag=False,\n",
    "                        folds=5\n",
    "                       )\n",
    "df['fold'] = dataset.fold\n",
    "df['full'] = [a for a in full_X.squeeze()]\n",
    "df.columns = ['target', 'h_target', 'derived', 'fold', 'base']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>h_target</th>\n",
       "      <th>derived</th>\n",
       "      <th>fold</th>\n",
       "      <th>base</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "      <td>mfcc_2_std  mfcc_2_mean  mfcc_2_noise  mfc...</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.03132703551358152, 0.049165900621142844, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>mfcc_2_std  mfcc_2_mean  mfcc_2_noise  mfc...</td>\n",
       "      <td>2</td>\n",
       "      <td>[-3.7501595140290916e-05, -0.00010058510417064...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>mfcc_2_std  mfcc_2_mean  mfcc_2_noise  mfc...</td>\n",
       "      <td>2</td>\n",
       "      <td>[-0.09333880784071064, -0.029133682284672446, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>mfcc_2_std  mfcc_2_mean  mfcc_2_noise  mfc...</td>\n",
       "      <td>2</td>\n",
       "      <td>[-0.0002920302234656099, -0.000457424359913381...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>mfcc_2_std  mfcc_2_mean  mfcc_2_noise  mfc...</td>\n",
       "      <td>2</td>\n",
       "      <td>[-0.0015238863621668303, -0.006384021990456582...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target  h_target                                            derived  fold  \\\n",
       "0      43         1      mfcc_2_std  mfcc_2_mean  mfcc_2_noise  mfc...     2   \n",
       "1       1         0      mfcc_2_std  mfcc_2_mean  mfcc_2_noise  mfc...     2   \n",
       "2      10         1      mfcc_2_std  mfcc_2_mean  mfcc_2_noise  mfc...     2   \n",
       "3      17         1      mfcc_2_std  mfcc_2_mean  mfcc_2_noise  mfc...     2   \n",
       "4      17         1      mfcc_2_std  mfcc_2_mean  mfcc_2_noise  mfc...     2   \n",
       "\n",
       "                                                base  \n",
       "0  [0.03132703551358152, 0.049165900621142844, 0....  \n",
       "1  [-3.7501595140290916e-05, -0.00010058510417064...  \n",
       "2  [-0.09333880784071064, -0.029133682284672446, ...  \n",
       "3  [-0.0002920302234656099, -0.000457424359913381...  \n",
       "4  [-0.0015238863621668303, -0.006384021990456582...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = df[df.fold != 1].reset_index(drop=True)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>h_target</th>\n",
       "      <th>derived</th>\n",
       "      <th>fold</th>\n",
       "      <th>base</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>mfcc_2_std  mfcc_2_mean  mfcc_2_noise  mfcc...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>mfcc_2_std  mfcc_2_mean  mfcc_2_noise  mfc...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.042354584992188686, -0.12557755392933506, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>mfcc_2_std  mfcc_2_mean  mfcc_2_noise  mfc...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.0071510895789658885, -0.008539486532633997...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>mfcc_2_std  mfcc_2_mean  mfcc_2_noise  mfc...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.3236247120233376, 0.23597562357962842, 0.49...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>mfcc_2_std  mfcc_2_mean  mfcc_2_noise  mfc...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.0004667657144710984, -0.000415740161992739...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target  h_target                                            derived  fold  \\\n",
       "0       0         0     mfcc_2_std  mfcc_2_mean  mfcc_2_noise  mfcc...     1   \n",
       "1      14         0      mfcc_2_std  mfcc_2_mean  mfcc_2_noise  mfc...     1   \n",
       "2      36         1      mfcc_2_std  mfcc_2_mean  mfcc_2_noise  mfc...     1   \n",
       "3      36         1      mfcc_2_std  mfcc_2_mean  mfcc_2_noise  mfc...     1   \n",
       "4      19         1      mfcc_2_std  mfcc_2_mean  mfcc_2_noise  mfc...     1   \n",
       "\n",
       "                                                base  \n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "1  [-0.042354584992188686, -0.12557755392933506, ...  \n",
       "2  [-0.0071510895789658885, -0.008539486532633997...  \n",
       "3  [0.3236247120233376, 0.23597562357962842, 0.49...  \n",
       "4  [-0.0004667657144710984, -0.000415740161992739...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = df[df.fold == 1].reset_index(drop=True)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train.drop(['target', 'h_target', 'fold'], axis=1)\n",
    "train_y = train[['h_target','target']]\n",
    "test_X = test.drop(['target', 'h_target', 'fold'], axis=1)\n",
    "test_y = test[['h_target','target']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mixd_mult.py\n"
     ]
    }
   ],
   "source": [
    "%%file mixd_mult.py\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import inspect\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Flatten, InputLayer\n",
    "from kapre.time_frequency import Melspectrogram, Spectrogram\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "SR=16000\n",
    "\n",
    "class Mixed_Multilayer(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, epochs=50, batch_size=128, validation_split=0.05,\n",
    "                       a_epochs=50, a_batch_size=128,\n",
    "                       i_epochs=50, i_batch_size=128,\n",
    "                       verbose=1, proc=None):\n",
    "        \n",
    "        args, _, _, values = inspect.getargvalues(inspect.currentframe())\n",
    "        values.pop(\"self\")\n",
    "\n",
    "        for arg, val in values.items():\n",
    "            setattr(self, arg, val)\n",
    "            pass\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        X_b = np.array([i for i in X['base']])\n",
    "        X_d = pd.concat([j for j in X['derived']], ignore_index=True)\n",
    "        \n",
    "        y_p = []\n",
    "        yy = []\n",
    "        a_y = []\n",
    "        i_y = []\n",
    "        for i in range(0, len(X)):\n",
    "            t_y = y.at[i, 'h_target']\n",
    "            t_yy = y.at[i, 'target']\n",
    "            length = len(X.at[i, 'derived'])\n",
    "            yy += [t_yy] * length\n",
    "            y_p += [t_y] * length\n",
    "            if t_y == 0:\n",
    "                a_y += [t_yy] * length\n",
    "            else:\n",
    "                i_y += [t_yy] * length\n",
    "        \n",
    "        if self.proc:\n",
    "            X_d = self.proc.fit_transform(X_d, yy)\n",
    "        \n",
    "        dims = X_d.shape[1]\n",
    "#         print(X_b.shape)\n",
    "#         print(X_d.shape)\n",
    "#         print(np.array(yy).shape)\n",
    "#         print(np.array(y_p).shape)\n",
    "        \n",
    "        \"\"\"Top layer of hierarchy\"\"\"\n",
    "        self.clf = KerasClassifier(build_fn=self.deep_net,\n",
    "                                   feature_count=SR * 5,\n",
    "                                   epochs=self.epochs, \n",
    "                                   batch_size=self.batch_size, \n",
    "                                   validation_split=self.validation_split,\n",
    "                                   verbose=self.verbose\n",
    "                                  )\n",
    "        self.clf.fit(X_b[:, np.newaxis, :], y.h_target)\n",
    "        \n",
    "        \"\"\"Animal Layer\"\"\"\n",
    "        self.a_clf = KerasClassifier(build_fn=self.deep_net_a,\n",
    "                                       feature_count=dims,\n",
    "                                       epochs=self.a_epochs, \n",
    "                                       batch_size=self.a_batch_size, \n",
    "                                       validation_split=self.validation_split,\n",
    "                                       verbose=self.verbose\n",
    "                                    )\n",
    "        \n",
    "        self.a_clf.fit(X_d[np.isin(y_p, 0)], a_y)\n",
    "        \n",
    "        \"\"\"Interacting Materials Layer\"\"\"\n",
    "        self.i_clf = KerasClassifier(build_fn=self.deep_net_i, \n",
    "                                       feature_count=dims,\n",
    "                                       epochs=self.i_epochs, \n",
    "                                       batch_size=self.i_batch_size, \n",
    "                                       validation_split=self.validation_split,\n",
    "                                       verbose=self.verbose\n",
    "                                    )\n",
    "        self.i_clf.fit(X_d[np.isin(y_p, 1)], i_y)\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def predict(self, X, y=None):\n",
    "        predictions=[]\n",
    "\n",
    "        X_b = np.array([i for i in X['base']])\n",
    "        X_d = X['derived']\n",
    "        \n",
    "        prob = self.clf.predict_proba(X_b[:, np.newaxis, :], verbose=0)\n",
    "\n",
    "        for i, d in enumerate(X_d):\n",
    "#             print(\"Document: \" + str(i))\n",
    "            # Transform before estimation\n",
    "            if self.proc:\n",
    "                d = self.proc.transform(d)\n",
    "            \n",
    "            if prob[i,0] > 0.75:\n",
    "                pred = self.a_clf.predict(d, verbose=0)\n",
    "            elif prob[i,1] > 0.75:\n",
    "                pred = self.i_clf.predict(d, verbose=0)\n",
    "            else:\n",
    "                a_prob = self.a_clf.predict_proba(d, verbose=0) * prob[i, 0]\n",
    "                i_prob = self.i_clf.predict_proba(d, verbose=0) * prob[i, 1]\n",
    "\n",
    "                a_prob = np.prod(a_prob, axis=0)\n",
    "                i_prob = np.prod(i_prob, axis=0)\n",
    "                \n",
    "                if(np.max(a_prob) > np.max(i_prob)):\n",
    "                    pred = self.a_clf.predict(d, verbose=0)\n",
    "                else:\n",
    "                    pred = self.i_clf.predict(d, verbose=0)\n",
    "\n",
    "            pred = np.bincount(pred).argmax()\n",
    "            predictions.append(pred)\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def predict_per_frame(self, X, y=None):\n",
    "        predictions=[]\n",
    "\n",
    "        X_b = np.array([i for i in X['base']])\n",
    "        X_d = X['derived']\n",
    "        \n",
    "        prob = self.clf.predict_proba(X_b[:, np.newaxis, :], verbose=0)\n",
    "\n",
    "        for i, d in enumerate(X_d):\n",
    "#             print(\"Document: \" + str(i))\n",
    "            # Transform before estimation\n",
    "            if self.proc:\n",
    "                d = self.proc.transform(d)\n",
    "            \n",
    "            if prob[i,0] > 0.75:\n",
    "                pred = self.a_clf.predict(d, verbose=0)\n",
    "            elif prob[i,1] > 0.75:\n",
    "                pred = self.i_clf.predict(d, verbose=0)\n",
    "            else:\n",
    "                a_prob = self.a_clf.predict_proba(d, verbose=0) * prob[i, 0]\n",
    "                i_prob = self.i_clf.predict_proba(d, verbose=0) * prob[i, 1]\n",
    "\n",
    "                a_prob = np.prod(a_prob, axis=0)\n",
    "                i_prob = np.prod(i_prob, axis=0)\n",
    "                \n",
    "                if(np.max(a_prob) > np.max(i_prob)):\n",
    "                    pred = self.a_clf.predict(d, verbose=0)\n",
    "                else:\n",
    "                    pred = self.i_clf.predict(d, verbose=0)\n",
    "\n",
    "            predictions.append(pred)\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def predict_proba(self, X, y=None):\n",
    "        \n",
    "        X_b = np.array([i for i in X['base']])\n",
    "        X_d = X['derived']\n",
    "        \n",
    "        prob = self.clf.predict_proba(X_b[:, np.newaxis, :], verbose=0)\n",
    "        \n",
    "        for i, d in enumerate(X_d):\n",
    "            if self.proc:\n",
    "                X_d = self.proc.transform(d)\n",
    "                \n",
    "            prob_a = np.multiply(np.prod(self.a_clf.predict_proba(X_d, verbose=0), axis=0),prob[i,0])\n",
    "#             print(prob_a.shape)\n",
    "\n",
    "            prob_i = np.multiply(np.prod(self.i_clf.predict_proba(X_d, verbose=0), axis=0),prob[i,0])\n",
    "#             print(prob_i.shape)\n",
    "#             print()\n",
    "        \n",
    "        probs = [None] * 50\n",
    "        for counter, j in enumerate(self.a_clf.classes_):\n",
    "            probs[j] = prob_a[counter]\n",
    "        for counter, j in enumerate(self.i_clf.classes_):\n",
    "            probs[j] = prob_i[counter]\n",
    "            \n",
    "        return np.array(probs)\n",
    "        \n",
    "    \n",
    "    def deep_net(self, feature_count):\n",
    "        # Create Model\n",
    "        model = Sequential()\n",
    "        model.add(Melspectrogram(\n",
    "            sr=SR,\n",
    "            n_mels=128,\n",
    "            power_melgram=1.0,\n",
    "            input_shape=(1, feature_count),\n",
    "            trainable_fb=False,\n",
    "            fmin = 800,\n",
    "            fmax = 8000\n",
    "        ))\n",
    "        model.add(Convolution2D(32, 9, 9, name='conv1', activation='relu'))\n",
    "        model.add(MaxPooling2D((25, 17)))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(32, activation='relu'))\n",
    "        model.add(Dense(32, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(32, activation='relu'))\n",
    "        model.add(Dense(32, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "\n",
    "        # Compile model\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "\n",
    "        return model\n",
    "\n",
    "    def deep_net_a(self, feature_count):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(64, activation='relu', input_shape=(50,)))\n",
    "        model.add(Dense(32, activation='relu'))\n",
    "        model.add(Dense(32, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(32, activation='relu'))\n",
    "        model.add(Dense(32, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(18, kernel_initializer='normal', activation='softmax'))\n",
    "\n",
    "        # Compile model\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "\n",
    "        return model\n",
    "\n",
    "    def deep_net_i(self, feature_count):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(64, activation='relu', input_shape=(50,)))\n",
    "        model.add(Dense(32, activation='relu'))\n",
    "        model.add(Dense(32, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(32, activation='relu'))\n",
    "        model.add(Dense(32, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(32, kernel_initializer='normal', activation='softmax'))\n",
    "\n",
    "        # Compile model\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "melspectrogram_1 (Melspectro (None, 128, 313, 1)       296064    \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 120, 305, 32)      2624      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 4, 17, 32)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2176)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                69664     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 371,553\n",
      "Trainable params: 75,489\n",
      "Non-trainable params: 296,064\n",
      "_________________________________________________________________\n",
      "Train on 1520 samples, validate on 80 samples\n",
      "Epoch 1/50\n",
      "1520/1520 [==============================] - 44s 29ms/step - loss: 0.6869 - acc: 0.5816 - val_loss: 0.6736 - val_acc: 0.7500\n",
      "Epoch 2/50\n",
      "1520/1520 [==============================] - 42s 28ms/step - loss: 0.6568 - acc: 0.6974 - val_loss: 0.6486 - val_acc: 0.7375\n",
      "Epoch 3/50\n",
      "1520/1520 [==============================] - 43s 29ms/step - loss: 0.6263 - acc: 0.7230 - val_loss: 0.6274 - val_acc: 0.7500\n",
      "Epoch 4/50\n",
      "1520/1520 [==============================] - 43s 28ms/step - loss: 0.5964 - acc: 0.7375 - val_loss: 0.6253 - val_acc: 0.7500\n",
      "Epoch 5/50\n",
      "1520/1520 [==============================] - 42s 28ms/step - loss: 0.5752 - acc: 0.7572 - val_loss: 0.5919 - val_acc: 0.7875\n",
      "Epoch 6/50\n",
      "1520/1520 [==============================] - 42s 28ms/step - loss: 0.5525 - acc: 0.7546 - val_loss: 0.5748 - val_acc: 0.8000\n",
      "Epoch 7/50\n",
      "1520/1520 [==============================] - 43s 28ms/step - loss: 0.5170 - acc: 0.7678 - val_loss: 0.5451 - val_acc: 0.7875\n",
      "Epoch 8/50\n",
      "1520/1520 [==============================] - 42s 28ms/step - loss: 0.5013 - acc: 0.7855 - val_loss: 0.5338 - val_acc: 0.8000\n",
      "Epoch 9/50\n",
      "1520/1520 [==============================] - 43s 28ms/step - loss: 0.4861 - acc: 0.7789 - val_loss: 0.5576 - val_acc: 0.7625\n",
      "Epoch 10/50\n",
      "1520/1520 [==============================] - 43s 28ms/step - loss: 0.4650 - acc: 0.7888 - val_loss: 0.4712 - val_acc: 0.8000\n",
      "Epoch 11/50\n",
      "1520/1520 [==============================] - 43s 28ms/step - loss: 0.4501 - acc: 0.7974 - val_loss: 0.4570 - val_acc: 0.8250\n",
      "Epoch 12/50\n",
      "1520/1520 [==============================] - 43s 28ms/step - loss: 0.4400 - acc: 0.8086 - val_loss: 0.5308 - val_acc: 0.7500\n",
      "Epoch 13/50\n",
      "1520/1520 [==============================] - 42s 28ms/step - loss: 0.4235 - acc: 0.8125 - val_loss: 0.5764 - val_acc: 0.7750\n",
      "Epoch 14/50\n",
      "1520/1520 [==============================] - 42s 28ms/step - loss: 0.4063 - acc: 0.8237 - val_loss: 0.4658 - val_acc: 0.7750\n",
      "Epoch 15/50\n",
      "1520/1520 [==============================] - 42s 28ms/step - loss: 0.3853 - acc: 0.8316 - val_loss: 0.4300 - val_acc: 0.8375\n",
      "Epoch 16/50\n",
      "1520/1520 [==============================] - 43s 28ms/step - loss: 0.3684 - acc: 0.8428 - val_loss: 0.4481 - val_acc: 0.8250\n",
      "Epoch 17/50\n",
      "1520/1520 [==============================] - 43s 28ms/step - loss: 0.3493 - acc: 0.8454 - val_loss: 0.5485 - val_acc: 0.8000\n",
      "Epoch 18/50\n",
      "1520/1520 [==============================] - 42s 28ms/step - loss: 0.3319 - acc: 0.8539 - val_loss: 0.5108 - val_acc: 0.7750\n",
      "Epoch 19/50\n",
      "1520/1520 [==============================] - 43s 28ms/step - loss: 0.3189 - acc: 0.8559 - val_loss: 0.5703 - val_acc: 0.8125\n",
      "Epoch 20/50\n",
      "1520/1520 [==============================] - 41s 27ms/step - loss: 0.3035 - acc: 0.8651 - val_loss: 0.6034 - val_acc: 0.7500\n",
      "Epoch 21/50\n",
      "1520/1520 [==============================] - 43s 28ms/step - loss: 0.2886 - acc: 0.8770 - val_loss: 0.4848 - val_acc: 0.8500\n",
      "Epoch 22/50\n",
      "1520/1520 [==============================] - 42s 28ms/step - loss: 0.2604 - acc: 0.8862 - val_loss: 0.7111 - val_acc: 0.8000\n",
      "Epoch 23/50\n",
      "1520/1520 [==============================] - 42s 28ms/step - loss: 0.2479 - acc: 0.8921 - val_loss: 0.6055 - val_acc: 0.8375\n",
      "Epoch 24/50\n",
      "1520/1520 [==============================] - 42s 28ms/step - loss: 0.2417 - acc: 0.9007 - val_loss: 0.7148 - val_acc: 0.7625\n",
      "Epoch 25/50\n",
      "1520/1520 [==============================] - 42s 28ms/step - loss: 0.2341 - acc: 0.9039 - val_loss: 0.5673 - val_acc: 0.8625\n",
      "Epoch 26/50\n",
      "1520/1520 [==============================] - 42s 28ms/step - loss: 0.1903 - acc: 0.9289 - val_loss: 0.6264 - val_acc: 0.8625\n",
      "Epoch 27/50\n",
      "1520/1520 [==============================] - 41s 27ms/step - loss: 0.1821 - acc: 0.9296 - val_loss: 0.6524 - val_acc: 0.8625\n",
      "Epoch 28/50\n",
      "1520/1520 [==============================] - 41s 27ms/step - loss: 0.1635 - acc: 0.9454 - val_loss: 0.7143 - val_acc: 0.8500\n",
      "Epoch 29/50\n",
      "1520/1520 [==============================] - 41s 27ms/step - loss: 0.2074 - acc: 0.9243 - val_loss: 0.8246 - val_acc: 0.8250\n",
      "Epoch 30/50\n",
      "1520/1520 [==============================] - 42s 27ms/step - loss: 0.1836 - acc: 0.9270 - val_loss: 0.6475 - val_acc: 0.8875\n",
      "Epoch 31/50\n",
      "1520/1520 [==============================] - 42s 28ms/step - loss: 0.1941 - acc: 0.9276 - val_loss: 0.7168 - val_acc: 0.8375\n",
      "Epoch 32/50\n",
      "1520/1520 [==============================] - 42s 28ms/step - loss: 0.1763 - acc: 0.9296 - val_loss: 0.6214 - val_acc: 0.8875\n",
      "Epoch 33/50\n",
      "1520/1520 [==============================] - 41s 27ms/step - loss: 0.1934 - acc: 0.9316 - val_loss: 0.6035 - val_acc: 0.8750\n",
      "Epoch 34/50\n",
      "1520/1520 [==============================] - 41s 27ms/step - loss: 0.1695 - acc: 0.9342 - val_loss: 0.6560 - val_acc: 0.8500\n",
      "Epoch 35/50\n",
      "1520/1520 [==============================] - 43s 28ms/step - loss: 0.1307 - acc: 0.9539 - val_loss: 0.7188 - val_acc: 0.8625\n",
      "Epoch 36/50\n",
      "1520/1520 [==============================] - 43s 28ms/step - loss: 0.1177 - acc: 0.9605 - val_loss: 0.7149 - val_acc: 0.8750\n",
      "Epoch 37/50\n",
      " 384/1520 [======>.......................] - ETA: 30s - loss: 0.1272 - acc: 0.9531"
     ]
    }
   ],
   "source": [
    "from mixd_mult import Mixed_Multilayer\n",
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "proc = Pipeline([\n",
    "    ('scaler', MinMaxScaler()),\n",
    "    ('feat_sel', SelectKBest(k=80, score_func=chi2)),\n",
    "    ('feat_red', PCA(n_components=50))\n",
    "])\n",
    "\n",
    "clf = Mixed_Multilayer(proc=proc)\n",
    "\n",
    "clf.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clf.predict(test_X)\n",
    "print(metrics.accuracy_score(test_y.target, pred))\n",
    "print(metrics.precision_score(test_y.target, pred, average='macro'))\n",
    "cm = metrics.confusion_matrix(test_y.target, pred)\n",
    "plt.figure(figsize=(20,20))\n",
    "plot_confusion_matrix(cm, classes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "import time\n",
    "from load_blockwise import load_file_blockwise\n",
    "\n",
    "predictions = []\n",
    "full_targets = []\n",
    "start_time = time.time()\n",
    "top_pred_probs = []\n",
    "pred_probs = []\n",
    "\n",
    "for data_iloc in range(0,len(test)):\n",
    "    s_time = time.time()\n",
    "\n",
    "    X = test.iloc[data_iloc][['derived', 'base']]\n",
    "    y = test.iloc[data_iloc]['target']\n",
    "\n",
    "#     Made for batch processing usually\n",
    "    X['base'] = X['base'][np.newaxis, :]\n",
    "    X['derived'] = [X['derived']]\n",
    "    \n",
    "    pred = clf.predict(X)\n",
    "    predictions.append(pred)\n",
    "    \n",
    "    full_targets.append(y)\n",
    "    print(\"\\tFile Time: \" + str(time.time() - s_time))\n",
    "\n",
    "print(\"\\tProcessing Time: \" + str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.accuracy_score(full_targets, predictions))\n",
    "print(metrics.precision_score(full_targets, predictions, average='macro'))\n",
    "cm = metrics.confusion_matrix(full_targets, predictions)\n",
    "plt.figure(figsize=(20,20))\n",
    "plot_confusion_matrix(cm, classes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_dataset(query_term):\n",
    "    h_l = mapping[query_term]\n",
    "    l_l = classes.index(query_term)\n",
    "    \n",
    "    predictions = []\n",
    "\n",
    "    for data_iloc in range(0,len(test)):\n",
    "        X = test.iloc[data_iloc][['derived', 'base']]\n",
    "        y = test.iloc[data_iloc]['target']\n",
    "\n",
    "    #     Made for batch processing usually\n",
    "        X['base'] = X['base'][np.newaxis, :]\n",
    "        X['derived'] = [X['derived']]\n",
    "\n",
    "        prob = clf.predict_proba(X)[l_l]\n",
    "        pred = clf.predict_per_frame(X)\n",
    "        \n",
    "        predictions.append({\n",
    "            'ds_id': data_iloc,\n",
    "            'prob': prob,\n",
    "            'prediction': pred\n",
    "        })\n",
    "        \n",
    "    predictions = pd.DataFrame(predictions).sort_values(by=['prob'], ascending=False).reset_index(drop=True)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "preds = query_dataset('dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking index\n",
    "def check_accuracy(preds, query, count):\n",
    "    misclass = []\n",
    "    \n",
    "    print(query)\n",
    "    for data_iloc in range(0, count):\n",
    "        row = preds.iloc[data_iloc]\n",
    "        if dataset.iloc[row.ds_id].target != classes.index(query):\n",
    "            misclass.append(classes[dataset.iloc[row.ds_id].target])\n",
    "            \n",
    "    print(misclass)\n",
    "    print()\n",
    "    return (count - len(misclass))/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n",
      "['hand_saw', 'glass_breaking', 'hand_saw', 'hand_saw', 'breathing', 'car_horn', 'coughing', 'hand_saw', 'sneezing', 'breathing', 'breathing', 'coughing', 'glass_breaking', 'car_horn', 'clock_tick', 'glass_breaking', 'rooster', 'hand_saw', 'rooster']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.05"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking index\n",
    "check_accuracy(preds, 'dog', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n",
      "['cow', 'glass_breaking', 'door_wood_knock', 'coughing', 'coughing', 'car_horn', 'mouse_click', 'sneezing', 'coughing', 'footsteps', 'drinking_sipping', 'glass_breaking', 'door_wood_knock', 'sneezing', 'insects']\n",
      "\n",
      "0.25\n",
      "rooster\n",
      "['coughing', 'dog', 'car_horn', 'insects', 'sneezing', 'car_horn', 'glass_breaking', 'coughing', 'glass_breaking', 'sneezing', 'water_drops', 'coughing', 'door_wood_knock', 'dog', 'can_opening', 'footsteps', 'door_wood_knock', 'glass_breaking']\n",
      "\n",
      "0.1\n",
      "pig\n",
      "['glass_breaking', 'car_horn', 'breathing', 'breathing', 'hand_saw', 'breathing', 'coughing', 'hand_saw', 'glass_breaking', 'hand_saw', 'car_horn', 'brushing_teeth', 'sneezing', 'rooster', 'coughing', 'dog', 'brushing_teeth', 'breathing', 'clock_tick']\n",
      "\n",
      "0.05\n",
      "cow\n",
      "['car_horn', 'sneezing', 'glass_breaking', 'dog', 'coughing', 'insects', 'glass_breaking', 'coughing', 'breathing', 'water_drops', 'rooster', 'dog', 'car_horn', 'footsteps', 'laughing', 'breathing', 'glass_breaking', 'sneezing']\n",
      "\n",
      "0.1\n",
      "frog\n",
      "['clock_alarm', 'footsteps', 'thunderstorm', 'water_drops', 'car_horn', 'glass_breaking', 'coughing', 'water_drops', 'dog', 'rooster', 'thunderstorm', 'fireworks', 'coughing', 'coughing', 'glass_breaking', 'rooster', 'sneezing', 'laughing']\n",
      "\n",
      "0.1\n",
      "cat\n",
      "['dog', 'car_horn', 'coughing', 'water_drops', 'glass_breaking', 'glass_breaking', 'footsteps', 'sneezing', 'coughing', 'insects', 'coughing', 'dog', 'laughing', 'rooster', 'rooster', 'door_wood_knock', 'car_horn', 'rooster', 'door_wood_knock', 'breathing']\n",
      "\n",
      "0.0\n",
      "hen\n",
      "['dog', 'car_horn', 'coughing', 'glass_breaking', 'footsteps', 'laughing', 'coughing', 'coughing', 'glass_breaking', 'sneezing', 'water_drops', 'rooster', 'insects', 'laughing', 'rooster', 'door_wood_knock', 'rooster', 'hand_saw', 'dog']\n",
      "\n",
      "0.05\n",
      "insects\n",
      "['car_horn', 'glass_breaking', 'coughing', 'cat', 'sneezing', 'dog', 'thunderstorm', 'cow', 'train', 'rooster', 'coughing', 'cat', 'breathing', 'glass_breaking', 'laughing']\n",
      "\n",
      "0.25\n",
      "sheep\n",
      "['car_horn', 'glass_breaking', 'sneezing', 'cow', 'coughing', 'breathing', 'breathing', 'car_horn', 'breathing', 'cow', 'insects', 'rooster', 'insects', 'coughing', 'train', 'sea_waves', 'glass_breaking']\n",
      "\n",
      "0.15\n",
      "crow\n",
      "['glass_breaking', 'car_horn', 'coughing', 'footsteps', 'dog', 'sneezing', 'coughing', 'rooster', 'breathing', 'glass_breaking', 'water_drops', 'breathing', 'rooster', 'chirping_birds', 'breathing', 'hand_saw', 'coughing', 'insects']\n",
      "\n",
      "0.1\n",
      "rain\n",
      "['helicopter', 'helicopter', 'crickets', 'vacuum_cleaner', 'crickets', 'vacuum_cleaner', 'washing_machine', 'crickets', 'helicopter', 'washing_machine', 'crickets', 'car_horn', 'crackling_fire']\n",
      "\n",
      "0.35\n",
      "sea_waves\n",
      "['car_horn', 'glass_breaking', 'breathing', 'car_horn', 'rooster', 'sneezing', 'breathing', 'breathing', 'breathing', 'wind', 'chirping_birds', 'airplane', 'rooster', 'cow', 'insects', 'chainsaw', 'laughing', 'glass_breaking']\n",
      "\n",
      "0.1\n",
      "crackling_fire\n",
      "['car_horn', 'footsteps', 'sneezing', 'dog', 'dog', 'glass_breaking', 'insects', 'coughing', 'coughing', 'rooster', 'glass_breaking', 'rooster', 'water_drops', 'breathing', 'coughing', 'engine']\n",
      "\n",
      "0.2\n",
      "crickets\n",
      "['helicopter', 'helicopter', 'vacuum_cleaner', 'vacuum_cleaner', 'rain', 'engine', 'helicopter', 'washing_machine', 'mouse_click', 'rain', 'washing_machine', 'vacuum_cleaner', 'insects', 'rain', 'rain']\n",
      "\n",
      "0.25\n",
      "chirping_birds\n",
      "['rooster', 'siren', 'rooster', 'brushing_teeth', 'glass_breaking', 'car_horn', 'siren', 'crickets', 'frog', 'washing_machine', 'crickets', 'crackling_fire', 'vacuum_cleaner', 'crickets', 'brushing_teeth', 'coughing']\n",
      "\n",
      "0.2\n",
      "water_drops\n",
      "['dog', 'car_horn', 'insects', 'footsteps', 'dog', 'sneezing', 'glass_breaking', 'coughing', 'coughing', 'coughing', 'sneezing', 'door_wood_knock', 'can_opening', 'sneezing', 'sneezing', 'door_wood_knock', 'door_wood_knock']\n",
      "\n",
      "0.15\n",
      "wind\n",
      "['airplane', 'car_horn', 'engine', 'airplane', 'train', 'thunderstorm', 'airplane', 'engine', 'church_bells', 'helicopter', 'train', 'insects', 'dog', 'airplane', 'sheep', 'church_bells', 'footsteps']\n",
      "\n",
      "0.15\n",
      "pouring_water\n",
      "['sneezing', 'car_horn', 'glass_breaking', 'rooster', 'rooster', 'coughing', 'glass_breaking', 'car_horn', 'breathing', 'breathing', 'dog', 'breathing', 'insects', 'car_horn', 'footsteps', 'brushing_teeth']\n",
      "\n",
      "0.2\n",
      "toilet_flush\n",
      "['car_horn', 'glass_breaking', 'sneezing', 'breathing', 'rooster', 'car_horn', 'breathing', 'breathing', 'rooster', 'glass_breaking', 'rooster', 'coughing', 'crow', 'laughing', 'pouring_water', 'breathing', 'washing_machine', 'clock_alarm', 'coughing']\n",
      "\n",
      "0.05\n",
      "thunderstorm\n",
      "['dog', 'car_horn', 'footsteps', 'rooster', 'water_drops', 'insects', 'dog', 'cow', 'rooster', 'sneezing', 'sneezing', 'rooster', 'door_wood_knock', 'cow', 'fireworks', 'glass_breaking', 'snoring']\n",
      "\n",
      "0.15\n",
      "crying_baby\n",
      "['glass_breaking', 'car_horn', 'coughing', 'dog', 'cat', 'breathing', 'laughing', 'rooster', 'sneezing', 'breathing', 'hand_saw', 'coughing', 'glass_breaking', 'car_horn', 'rooster', 'footsteps']\n",
      "\n",
      "0.2\n",
      "sneezing\n",
      "['dog', 'glass_breaking', 'car_horn', 'coughing', 'can_opening', 'glass_breaking', 'glass_breaking', 'coughing', 'car_horn', 'clock_alarm', 'insects', 'breathing', 'water_drops', 'door_wood_knock']\n",
      "\n",
      "0.3\n",
      "clapping\n",
      "['glass_breaking', 'coughing', 'breathing', 'sneezing', 'car_horn', 'hand_saw', 'breathing', 'glass_breaking', 'breathing', 'coughing', 'dog', 'glass_breaking', 'car_horn', 'rooster', 'clock_alarm', 'glass_breaking', 'car_horn', 'footsteps', 'laughing', 'pouring_water']\n",
      "\n",
      "0.0\n",
      "breathing\n",
      "['glass_breaking', 'car_horn', 'car_horn', 'glass_breaking', 'sneezing', 'rooster', 'coughing', 'hand_saw', 'coughing', 'clock_alarm', 'glass_breaking', 'hand_saw', 'engine', 'hand_saw', 'snoring', 'glass_breaking']\n",
      "\n",
      "0.2\n",
      "coughing\n",
      "['glass_breaking', 'dog', 'sneezing', 'car_horn', 'glass_breaking', 'can_opening', 'car_horn', 'insects', 'sneezing', 'sneezing', 'sneezing', 'door_wood_knock', 'water_drops', 'sneezing', 'breathing', 'door_wood_knock', 'sneezing']\n",
      "\n",
      "0.15\n",
      "footsteps\n",
      "['car_horn', 'dog', 'coughing', 'sneezing', 'glass_breaking', 'coughing', 'dog', 'coughing', 'glass_breaking', 'rooster', 'insects', 'breathing', 'laughing', 'water_drops', 'door_wood_knock', 'rooster', 'door_wood_knock', 'car_horn', 'breathing']\n",
      "\n",
      "0.05\n",
      "laughing\n",
      "['coughing', 'car_horn', 'glass_breaking', 'glass_breaking', 'coughing', 'dog', 'sneezing', 'insects', 'coughing', 'dog', 'footsteps', 'water_drops', 'rooster', 'rooster', 'car_horn', 'rooster', 'door_wood_knock', 'door_wood_knock', 'rooster']\n",
      "\n",
      "0.05\n",
      "brushing_teeth\n",
      "['car_horn', 'glass_breaking', 'sneezing', 'rooster', 'coughing', 'breathing', 'pouring_water', 'rooster', 'glass_breaking', 'breathing', 'breathing', 'car_horn', 'coughing', 'dog', 'footsteps']\n",
      "\n",
      "0.25\n",
      "snoring\n",
      "['glass_breaking', 'car_horn', 'breathing', 'breathing', 'sneezing', 'breathing', 'coughing', 'car_horn', 'glass_breaking', 'coughing', 'dog', 'rooster', 'insects', 'washing_machine', 'cow', 'hand_saw', 'coughing']\n",
      "\n",
      "0.15\n",
      "drinking_sipping\n",
      "['dog', 'car_horn', 'sneezing', 'glass_breaking', 'insects', 'coughing', 'dog', 'footsteps', 'water_drops', 'coughing', 'coughing', 'sneezing', 'car_horn', 'can_opening', 'glass_breaking', 'water_drops', 'door_wood_knock', 'rooster', 'door_wood_knock', 'sneezing']\n",
      "\n",
      "0.0\n",
      "door_wood_knock\n",
      "['dog', 'car_horn', 'sneezing', 'coughing', 'glass_breaking', 'coughing', 'footsteps', 'water_drops', 'insects', 'dog', 'sneezing', 'coughing', 'dog', 'drinking_sipping', 'can_opening', 'sneezing']\n",
      "\n",
      "0.2\n",
      "mouse_click\n",
      "['dog', 'coughing', 'car_horn', 'glass_breaking', 'sneezing', 'coughing', 'glass_breaking', 'insects', 'footsteps', 'breathing', 'coughing', 'can_opening', 'car_horn', 'dog', 'sneezing', 'water_drops', 'sneezing', 'rooster', 'laughing', 'glass_breaking']\n",
      "\n",
      "0.0\n",
      "keyboard_typing\n",
      "['coughing', 'car_horn', 'sneezing', 'coughing', 'glass_breaking', 'dog', 'glass_breaking', 'breathing', 'footsteps', 'laughing', 'coughing', 'insects', 'rooster', 'rooster', 'dog', 'breathing', 'car_horn', 'hand_saw', 'can_opening']\n",
      "\n",
      "0.05\n",
      "door_wood_creaks\n",
      "['glass_breaking', 'car_horn', 'sneezing', 'dog', 'glass_breaking', 'coughing', 'breathing', 'coughing', 'breathing', 'breathing', 'coughing', 'car_horn', 'car_horn', 'hand_saw', 'door_wood_knock', 'breathing', 'door_wood_knock', 'rooster', 'door_wood_knock', 'sneezing']\n",
      "\n",
      "0.0\n",
      "can_opening\n",
      "['dog', 'coughing', 'sneezing', 'glass_breaking', 'insects', 'car_horn', 'water_drops', 'coughing', 'footsteps', 'car_horn', 'sneezing', 'glass_breaking', 'coughing', 'sneezing', 'sneezing', 'water_drops', 'breathing', 'sneezing', 'glass_breaking']\n",
      "\n",
      "0.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "washing_machine\n",
      "['car_horn', 'dog', 'footsteps', 'glass_breaking', 'sneezing', 'coughing', 'dog', 'glass_breaking', 'crickets', 'breathing', 'wind', 'coughing', 'rooster', 'crickets', 'wind', 'rooster', 'engine', 'breathing']\n",
      "\n",
      "0.1\n",
      "vacuum_cleaner\n",
      "['glass_breaking', 'breathing', 'breathing', 'breathing', 'crickets', 'car_horn', 'car_horn', 'washing_machine', 'breathing', 'sneezing', 'crickets', 'rooster', 'wind', 'wind', 'coughing', 'wind']\n",
      "\n",
      "0.2\n",
      "clock_alarm\n",
      "['glass_breaking', 'coughing', 'breathing', 'breathing', 'sneezing', 'hand_saw', 'car_horn', 'glass_breaking', 'breathing', 'coughing', 'dog', 'glass_breaking', 'glass_breaking', 'car_horn', 'rooster', 'car_horn', 'pouring_water', 'laughing', 'footsteps']\n",
      "\n",
      "0.05\n",
      "clock_tick\n",
      "['dog', 'car_horn', 'coughing', 'footsteps', 'dog', 'glass_breaking', 'coughing', 'insects', 'coughing', 'sneezing', 'sneezing', 'water_drops', 'can_opening', 'door_wood_knock', 'door_wood_knock', 'dog', 'mouse_click', 'mouse_click']\n",
      "\n",
      "0.1\n",
      "glass_breaking\n",
      "['sneezing', 'coughing', 'breathing', 'breathing', 'dog', 'car_horn', 'breathing', 'car_horn', 'coughing', 'pouring_water', 'hand_saw', 'car_horn', 'hand_saw', 'coughing', 'insects', 'sneezing']\n",
      "\n",
      "0.2\n",
      "helicopter\n",
      "['car_horn', 'dog', 'airplane', 'airplane', 'glass_breaking', 'footsteps', 'sneezing', 'wind', 'cow', 'thunderstorm', 'dog', 'rooster', 'engine', 'rooster', 'engine', 'train', 'wind', 'airplane', 'airplane', 'train']\n",
      "\n",
      "0.0\n",
      "chainsaw\n",
      "['glass_breaking', 'breathing', 'breathing', 'breathing', 'car_horn', 'car_horn', 'breathing', 'sneezing', 'rooster', 'cat', 'coughing', 'glass_breaking', 'airplane', 'crow', 'cow', 'sea_waves', 'wind', 'rooster']\n",
      "\n",
      "0.1\n",
      "siren\n",
      "['cat', 'car_horn', 'rooster', 'cat', 'sneezing', 'cow', 'footsteps', 'dog', 'chainsaw', 'sheep', 'sheep', 'glass_breaking', 'rooster', 'insects', 'wind', 'crying_baby']\n",
      "\n",
      "0.2\n",
      "car_horn\n",
      "['sneezing', 'dog', 'glass_breaking', 'glass_breaking', 'coughing', 'breathing', 'coughing', 'dog', 'insects', 'rooster', 'breathing', 'footsteps', 'cow', 'breathing', 'rooster', 'water_drops', 'coughing']\n",
      "\n",
      "0.15\n",
      "engine\n",
      "['car_horn', 'sneezing', 'glass_breaking', 'washing_machine', 'coughing', 'glass_breaking', 'breathing', 'dog', 'crickets', 'breathing', 'rooster', 'helicopter', 'footsteps', 'crickets', 'car_horn', 'helicopter', 'coughing', 'breathing', 'rooster', 'crickets']\n",
      "\n",
      "0.0\n",
      "train\n",
      "['car_horn', 'dog', 'clock_tick', 'footsteps', 'rooster', 'pig', 'cow', 'sneezing', 'insects', 'sheep', 'insects', 'engine', 'rooster', 'washing_machine', 'laughing', 'glass_breaking', 'sheep', 'insects']\n",
      "\n",
      "0.1\n",
      "church_bells\n",
      "['car_horn', 'crying_baby', 'siren', 'crying_baby', 'cow', 'cat', 'dog', 'rooster', 'sneezing', 'footsteps', 'sheep', 'airplane', 'airplane', 'pig', 'sheep', 'pig']\n",
      "\n",
      "0.2\n",
      "airplane\n",
      "['car_horn', 'church_bells', 'thunderstorm', 'sheep', 'church_bells', 'glass_breaking', 'cow', 'sheep', 'thunderstorm', 'insects', 'dog', 'rooster', 'sheep', 'car_horn', 'train', 'sneezing']\n",
      "\n",
      "0.2\n",
      "fireworks\n",
      "['car_horn', 'dog', 'dog', 'sneezing', 'glass_breaking', 'footsteps', 'insects', 'coughing', 'coughing', 'coughing', 'water_drops', 'rooster', 'glass_breaking', 'breathing', 'car_horn', 'door_wood_knock', 'sneezing', 'laughing', 'can_opening', 'drinking_sipping']\n",
      "\n",
      "0.0\n",
      "hand_saw\n",
      "['glass_breaking', 'breathing', 'car_horn', 'coughing', 'sneezing', 'breathing', 'breathing', 'coughing', 'glass_breaking', 'car_horn', 'clock_tick', 'glass_breaking', 'rooster', 'rooster', 'dog']\n",
      "\n",
      "0.25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.12900000000000003"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies = []\n",
    "for cls in classes:\n",
    "    preds = query_dataset(cls)\n",
    "    acc = check_accuracy(preds, cls, 20)\n",
    "    print(acc)\n",
    "    accuracies.append(acc)\n",
    "    \n",
    "np.average(accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High-Level Shallow Nets\n",
    "Train binary shallow nets for high level categories(animals, natural, human, domestic, urban)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Convolution2D, MaxPooling2D, Flatten, InputLayer\n",
    "import keras.metrics as kmet\n",
    "from kapre.time_frequency import Melspectrogram, Spectrogram\n",
    "from kapre.utils import Normalization2D\n",
    "from kapre.augmentation import AdditiveNoise\n",
    "\n",
    "ensemble_num = 10\n",
    "num_hidden_neurons = 10\n",
    "dropout = 0.25\n",
    "\n",
    "epochs = 50\n",
    "batch = 128\n",
    "\n",
    "def deep_net():\n",
    "    # Create Model\n",
    "    model = Sequential()\n",
    "    model.add(Melspectrogram(\n",
    "        sr=SR,\n",
    "        n_mels=128,\n",
    "        power_melgram=1.0,\n",
    "        input_shape=(1, SR * 5),\n",
    "        trainable_fb=False,\n",
    "        fmin = 800,\n",
    "        fmax = 8000\n",
    "    ))\n",
    "    model.add(Convolution2D(32, 9, 9, name='conv1', activation='relu'))\n",
    "    model.add(MaxPooling2D((25, 17)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy', kmet.mae])\n",
    "    \n",
    "    model.summary()\n",
    "\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Pipeline([\n",
    "    ('classify', KerasClassifier(build_fn=deep_net, \n",
    "                       epochs=epochs, \n",
    "                       batch_size=batch, \n",
    "                       validation_split=0.15)\n",
    "    )\n",
    "])\n",
    "\n",
    "clf.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Low-Level Deep Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Animal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_net_a():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_shape=(30,)))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(18, kernel_initializer='normal', activation='softmax'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "clf_a = Pipeline([\n",
    "        ('scaler', MinMaxScaler()),\n",
    "        ('feat_sel', SelectKBest(k=80, score_func=chi2)),\n",
    "        ('feat_red', PCA(n_components=30)),\n",
    "        ('classify', KerasClassifier(build_fn=deep_net_a, \n",
    "                           epochs=50, \n",
    "                           batch_size=35, \n",
    "                           validation_split=0.05)\n",
    "        )\n",
    "])\n",
    "\n",
    "print(train_dy[train['h_target'] == 0].values.squeeze().shape)\n",
    "\n",
    "clf_a.fit(train_dX[train['h_target'] == 0], train_dy[train['h_target'] == 0].values.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interacting Material"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_net_i():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_shape=(30,)))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(32, kernel_initializer='normal', activation='softmax'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_i = Pipeline([\n",
    "        ('scaler', MinMaxScaler()),\n",
    "        ('feat_sel', SelectKBest(k=80, score_func=chi2)),\n",
    "        ('feat_red', PCA(n_components=30)),\n",
    "        ('classify', KerasClassifier(build_fn=deep_net_i, \n",
    "                                              epochs=50, \n",
    "                                              batch_size=35,\n",
    "                                              validation_split=0.15)\n",
    "        )\n",
    "])\n",
    "\n",
    "clf_i.fit(train_dX[train['h_target'] == 1], train_dy[train['h_target'] == 1].values.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_a = []\n",
    "for name in np.unique(test_yy[test_y == 0]):\n",
    "    names_a.append(classes[name])\n",
    "names_i = []\n",
    "for name in np.unique(test_yy[test_y == 1]):\n",
    "    names_i.append(classes[name])\n",
    "print(names_a)\n",
    "print(names_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred = clf.predict(test_X)\n",
    "print(metrics.accuracy_score(test_y, pred))\n",
    "cm = metrics.confusion_matrix(test_y, pred)\n",
    "plt.figure(figsize=(10,10))\n",
    "plot_confusion_matrix(cm, h_classes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clf_a.predict(test_dX[test['h_target'] == 0])\n",
    "print(metrics.accuracy_score(test_dy[test['h_target'] == 0].values.squeeze(), pred))\n",
    "cm = metrics.confusion_matrix(test_dy[test['h_target'] == 0].values.squeeze(), pred)\n",
    "plt.figure(figsize=(10,10))\n",
    "plot_confusion_matrix(cm, names_a)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clf_i.predict(test_dX[test['h_target'] == 1])\n",
    "print(metrics.accuracy_score(test_dy[test['h_target'] == 1].values.squeeze(), pred))\n",
    "cm = metrics.confusion_matrix(test_dy[test['h_target'] == 1].values.squeeze(), pred)\n",
    "plt.figure(figsize=(10,10))\n",
    "plot_confusion_matrix(cm, names_i)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Evaluation\n",
    "We combine the classifiers to determine overall performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = []\n",
    "full_targets = []\n",
    "start_time = time.time()\n",
    "top_pred_probs = []\n",
    "pred_probs = []\n",
    "\n",
    "test = dataset[dataset.fold == 1].reset_index()\n",
    "    \n",
    "for data_iloc in range(0,len(test)):\n",
    "    s_time = time.time()\n",
    "    x_file_pp = ps.preprocess_file(test.iloc[data_iloc].filename,\n",
    "                                                 blocksize=blocksize,\n",
    "                                                 overlap=overlap)\n",
    "    \n",
    "    x_file = load_file_blockwise(test.iloc[data_iloc].filename, orig_blocksize, orig_overlap)\n",
    "    \n",
    "    y_file = test.iloc[data_iloc].h_target\n",
    "    yy_file = test.iloc[data_iloc].target\n",
    "    \n",
    "    pred = clf.predict(x_file, verbose=0)[:,0]\n",
    "    top_pred_probs.append(clf.predict_proba(x_file))\n",
    "    prob = np.average(top_pred_probs[-1], axis=0)\n",
    "    if prob[0] > 0.5:\n",
    "        pred = clf_a.predict(x_file_pp, verbose=0)\n",
    "        b = Counter(pred)\n",
    "        predictions.append(b.most_common(1)[0][0])\n",
    "        pred_probs.append(clf_a.predict_proba(x_file_pp))\n",
    "    else:\n",
    "        pred = clf_i.predict(x_file_pp, verbose=0)\n",
    "        b = Counter(pred)\n",
    "        predictions.append(b.most_common(1)[0][0])\n",
    "        pred_probs.append(clf_i.predict_proba(x_file_pp))\n",
    "    \n",
    "    full_targets.append(yy_file)\n",
    "    print(\"\\tFile Time: \" + str(time.time() - s_time))\n",
    "\n",
    "print(\"\\tProcessing Time: \" + str(time.time() - start_time))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.accuracy_score(full_targets, predictions))\n",
    "print(metrics.precision_score(full_targets, predictions, average='macro'))\n",
    "cm = metrics.confusion_matrix(full_targets, predictions)\n",
    "plt.figure(figsize=(20,20))\n",
    "plot_confusion_matrix(cm, classes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_dataset(query_term):\n",
    "    h_l = mapping[query_term]\n",
    "    l_l = classes.index(query_term)\n",
    "    \n",
    "    predictions = []\n",
    "\n",
    "    for data_iloc in range(0,len(test)):\n",
    "        x_file_pp = ps.preprocess_file(test.iloc[data_iloc].filename,\n",
    "                                                     blocksize=blocksize,\n",
    "                                                     overlap=overlap)\n",
    "\n",
    "        x_file = load_file_blockwise(test.iloc[data_iloc].filename, orig_blocksize, orig_overlap)\n",
    "\n",
    "        y_file = test.iloc[data_iloc].h_target\n",
    "        yy_file = test.iloc[data_iloc].target\n",
    "\n",
    "        pred = clf.predict(x_file, verbose=0)[:,0]\n",
    "        top_pred_probs.append(clf.predict_proba(x_file))\n",
    "        prob = np.average(top_pred_probs[-1], axis=0)\n",
    "        if prob[0] > 0.5:\n",
    "            pred = clf_a.predict(x_file_pp, verbose=0)\n",
    "            b = Counter(pred)\n",
    "            predictions.append(b.most_common(1)[0][0])\n",
    "            pred_probs.append(clf_a.predict_proba(x_file_pp))\n",
    "        else:\n",
    "            pred = clf_i.predict(x_file_pp, verbose=0)\n",
    "            b = Counter(pred)\n",
    "            predictions.append(b.most_common(1)[0][0])\n",
    "            pred_probs.append(clf_i.predict_proba(x_file_pp))\n",
    "\n",
    "        \n",
    "        prob = clf.predict_proba(x_file)[l_l]\n",
    "        pred = clf.predict(x_file[np.newaxis, :, :, :])\n",
    "        \n",
    "        predictions.append({\n",
    "            'file': test.iloc[data_iloc].filename,\n",
    "            'ds_id': data_iloc,\n",
    "            'prob': prob,\n",
    "            'prediction': pred\n",
    "        })\n",
    "        \n",
    "    predictions = pd.DataFrame(predictions).sort_values(by=['prob'], ascending=False).reset_index(drop=True)\n",
    "    return predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
